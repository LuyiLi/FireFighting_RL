{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:07:48.221740103Z",
     "start_time": "2023-10-27T10:07:48.128129219Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mACNet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ACNet\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from ACNet import ACNet\n",
    "from FireFightingEnv import FFEnv\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "dev_list = device_lib.list_local_devices()\n",
    "print(dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __init__(self, observation_size=11, world0=None, goals0=None, DIAGONAL_MOVEMENT=False, SIZE=(10,40), PROB=(0,.5))\n",
    "# TODO: add support of size, prob, etc, to initialize FFEnv Momnent lah\n",
    "# Set agengt_num=1 for simplification. How to design a network support multi-agent? FIXED,follow idea 'batch'\n",
    "agent_num = 3\n",
    "\n",
    "env = FFEnv(agent_num=agent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.mapEnv.plotAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.array(env.observe_space)\n",
    "\n",
    "# obstacle map of agent[0]\n",
    "print(obs[0, 0], \"\\n\")\n",
    "# fire map of agent[0]\n",
    "print(obs[0, 1], \"\\n\")\n",
    "# agent map of agent[0]\n",
    "print(obs[0, 2], \"\\n\")\n",
    "# flammable map of agent[0]\n",
    "print(obs[0, 3], \"\\n\")\n",
    "# reserved water of agent[0]\n",
    "print(obs[0, 4], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# list & NumPy array, that's a question (re: array is the best)\n",
    "inputs_raw = np.array(obs[:, :4])\n",
    "inputs = []\n",
    "for map in inputs_raw:\n",
    "    reshaped_map = [np.array(arr) for arr in map]\n",
    "    inputs.append(reshaped_map)\n",
    "inputs = np.array(inputs)\n",
    "print(inputs.shape)\n",
    "\n",
    "water_res = np.array(obs[:, 4]).reshape(-1,1)\n",
    "print(water_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action size: 4 direction movements, stop, 4 direction spraying with 5 kinds of range each direction\n",
    "# consider a_size = 5 first (4 direction movements + stop)\n",
    "# a_size = 5+4*5\n",
    "a_size = 5\n",
    "\n",
    "trainer = tf.contrib.opt.NadamOptimizer(learning_rate=1e-4)\n",
    "\n",
    "# __init__(self, scope, a, trainer, TRAINING, GRID_SIZE)\n",
    "net = ACNet( \"global\", a_size, trainer, True, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "sess.run(tf.global_variables_initializer()) # setting random weights for all layers of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_state = net.state_init # (zeros(1,128), zeros(1,128))\n",
    "\n",
    "feed_dict = {\n",
    "    net.inputs: inputs,\n",
    "    net.water_res: water_res,\n",
    "    net.state_in[0]: rnn_state[0],\n",
    "    net.state_in[1]: rnn_state[1],\n",
    "}\n",
    "\n",
    "policy = sess.run(net.policy, feed_dict = feed_dict)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "episode_rollout = []\n",
    "iteration = 10\n",
    "\n",
    "actions = []\n",
    "for i in range(3):\n",
    "    actions.append([[], [], [], []])\n",
    "\n",
    "for t in range(iteration):\n",
    "    # 4 parameter to describe 1 action\n",
    "    # Moving direction, spraying distance, spraying direction, help bacon\n",
    "    for a in actions:\n",
    "        a[0] = np.random.choice([0, 1, 2, 3, 4])\n",
    "        a[1] = np.random.choice([0, 1, 2, 3, 4, 5])\n",
    "        a[2] = np.random.choice([0, 1, 2, 3, 4])\n",
    "        a[3] = np.random.choice([0, 1])\n",
    "\n",
    "    s1, rewards, done = env.step(actions)\n",
    "\n",
    "    state = np.array(state)\n",
    "    maps_raw = np.array(state[:, :4])\n",
    "    maps = []\n",
    "    for map in maps_raw:\n",
    "        reshaped_map = [np.array(arr) for arr in map]\n",
    "        maps.append(reshaped_map)\n",
    "    maps = np.array(maps)\n",
    "    \n",
    "    water = np.array(state[:, 4]).reshape(-1,1)\n",
    "\n",
    "    episode_rollout.append([maps, water, actions, rewards])\n",
    "    state = s1[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, channel, map_size[0], map_size[1]\n",
    "inputs    = np.array([item[0] for item in episode_rollout]).reshape(iteration*agent_num, 4, 11, 11)\n",
    "# batch_size, water_reserved\n",
    "water_res = np.array([item[1] for item in episode_rollout]).reshape(iteration*agent_num, 1)\n",
    "# batch_size, action_parameters\n",
    "actions   = np.array([item[2] for item in episode_rollout]).reshape(iteration*agent_num, 4)\n",
    "# batch_size, reward\n",
    "rewards   = np.array([item[3] for item in episode_rollout]).reshape(iteration*agent_num, 1)\n",
    "print(np.shape(inputs))\n",
    "print(np.shape(water_res))\n",
    "print(np.shape(actions))\n",
    "print(np.shape(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs    = rollout[:,0] # 10x3x11x11x4\n",
    "# water_res = rollout[:,1] # 10x3x1\n",
    "# actions   = rollout[:,2] # 10x3x4\n",
    "# rewards   = rollout[:,3] # 10x3x1 FIXME!!!\n",
    "# valids    = rollout[:,4] # 10x3x25 FIXME!!!\n",
    "\n",
    "\n",
    "# In practice: calculate long-term cumulative discounted rewards from episode rewards here\n",
    "# --> net.target_v\n",
    "\n",
    "# In practice: calculate advantages estimates (TD-error) here\n",
    "# --> net.advantages\n",
    "\n",
    "\n",
    "rnn_state = net.state_init\n",
    "feed_dict = {\n",
    "    net.inputs:      np.stack(inputs),\n",
    "    net.water_res:   np.stack(water_res),\n",
    "    net.actions:     np.stack(actions),\n",
    "\n",
    "    net.target_v:    np.stack(rewards),\n",
    "    net.advantages:  np.stack(rewards),\n",
    "\n",
    "    net.state_in[0]: rnn_state[0],\n",
    "    net.state_in[1]: rnn_state[1]\n",
    "}\n",
    "\n",
    "v_l, p_l, valid_l, g_n, _ = sess.run([net.value_loss,\n",
    "                                      net.policy_loss,\n",
    "                                      net.grad_norms,\n",
    "                                      net.apply_grads], feed_dict=feed_dict)\n",
    "\n",
    "print(v_l, p_l, valid_l, g_n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
