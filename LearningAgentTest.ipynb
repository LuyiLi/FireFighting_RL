{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.signal as signal\n",
    "import os\n",
    "import pickle\n",
    "import imageio\n",
    "\n",
    "#import sapp_gym\n",
    "from ACNet_pytorch import ACNet\n",
    "from ACNet_pytorch import ACNetLoss\n",
    "#from torchviz import make_dot\n",
    "import AgentState\n",
    "from FireFightingEnv import FFEnv\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "#model = \n",
    "#model = model.to(device)\n",
    "\n",
    "\n",
    "def make_gif(images, fname, duration=2, true_image=False,salience=False,salIMGS=None):\n",
    "    imageio.mimwrite(fname,images,subrectangles=True)\n",
    "    print(\"wrote gif\")\n",
    "\n",
    "def discount(x, gamma):\n",
    "    return signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning parameters\n",
    "max_episode_length     = 128\n",
    "episode_count          = 0\n",
    "EPISODE_START          = episode_count\n",
    "gamma                  = .95 # discount rate for advantage estimation and reward discounting\n",
    "#moved network parameters to ACNet.py\n",
    "EXPERIENCE_BUFFER_SIZE = 128\n",
    "GRID_SIZE              = 11 #the size of the FOV grid to apply to each agent\n",
    "ENVIRONMENT_SIZE       = (15,15)#the total size of the environment (length of one side)\n",
    "OBSTACLE_DENSITY       = (0,.3) #range of densities\n",
    "DIAG_MVMT              = False # Diagonal movements allowed?\n",
    "a_size                 = 4 + 1 + 4 + 1 # action size: 4 direction movements, stop, 4 direction spraying, and go back to water supply station.\n",
    "SUMMARY_WINDOW         = 10\n",
    "LR_Q                   = 8.e-5\n",
    "load_model             = False\n",
    "RESET_TRAINER          = False\n",
    "model_path             = 'model_sapp_vanilla'\n",
    "gifs_path              = 'gifs_sapp_vanilla'\n",
    "train_path             = 'train_sapp_vanilla'\n",
    "GLOBAL_NET_SCOPE       = 'global'\n",
    "agent_num              = 3\n",
    "\n",
    "# Simulation options\n",
    "FULL_HELP              = False\n",
    "OUTPUT_GIFS            = True\n",
    "SAVE_EPISODE_BUFFER    = False\n",
    "\n",
    "# Testing\n",
    "TRAINING               = True\n",
    "GREEDY                 = False\n",
    "NUM_EXPS               = 100\n",
    "MODEL_NUMBER           = 140500\n",
    "\n",
    "# Shared arrays for tensorboard\n",
    "episode_rewards        = []\n",
    "episode_lengths        = []\n",
    "episode_mean_values    = []\n",
    "episode_invalid_ops    = []\n",
    "rollouts               = None\n",
    "printQ                 = False # (for headless)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-5b8f55391b81>, line 127)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-5b8f55391b81>\"\u001b[1;36m, line \u001b[1;32m127\u001b[0m\n\u001b[1;33m    actions,~,~,~ =  self.global_network.forward(inputs,water_res)\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Worker:\n",
    "    def __init__(self, game, global_network):\n",
    "        self.env = game\n",
    "        self.global_network = global_network\n",
    "\n",
    "        self.nextGIF = episode_count # For GIFs output\n",
    "\n",
    "    def train(self, rollout, gamma, bootstrap_value, rnn_state0, imitation=False):\n",
    "        #tf:(self, rollout, sess, gamma, bootstrap_value, rnn_state0, imitation=False)\n",
    "        global episode_count\n",
    "\n",
    "        # [s[0],s[1],a,r,s1,d,v[0,0],train_valid]\n",
    "        rollout     = np.array(rollout)\n",
    "        observ      = torch.from_numpy(rollout[:,0])\n",
    "        #goal_vec    = torch.from_numpy(rollout[:,1])\n",
    "        actions     = torch.from_numpy(rollout[:,2])\n",
    "        rewards     = torch.from_numpy(rollout[:,3])\n",
    "        policy      = torch.from_numpy(rollout[:,4])\n",
    "        values      = torch.from_numpy(rollout[:,6])\n",
    "        #valids      = torch.from_numpy(rollout[:,7])\n",
    "\n",
    "        # Here we take the rewards and values from the rollout, and use them to \n",
    "        # generate the advantage and discounted returns. (With bootstrapping)\n",
    "        # The advantage function uses \"Generalized Advantage Estimation\"\n",
    "        self.rewards_plus = torch.cat((rewards, bootstrap_value.unsqueeze(0)))\n",
    "        #self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "        discounted_rewards = discount(self.rewards_plus,gamma)[:-1]\n",
    "        self.value_plus = torch.cat((values, bootstrap_value.unsqueeze(0)))\n",
    "        #self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "        advantages = discount(advantages,gamma)\n",
    "\n",
    "        # Generate network statistics to periodically save\n",
    "        #feed_dict = {\n",
    "        #    self.global_network.target_v:np.stack(discounted_rewards),\n",
    "        #    self.global_network.inputs:np.stack(observ),\n",
    "        #    self.global_network.goal_pos:np.stack(goal_vec),\n",
    "        #    self.global_network.actions:actions,\n",
    "        #    self.global_network.train_valid:np.stack(valids),\n",
    "        #    self.global_network.advantages:advantages,\n",
    "        #    self.global_network.state_in[0]:rnn_state0[0],\n",
    "        #    self.global_network.state_in[1]:rnn_state0[1]\n",
    "        #}\n",
    " \n",
    "        ## Modifing grad\n",
    "        \n",
    "        target_v = torch.stack(discounted_rewards)\n",
    "        #inputs = torch.stack(observ)\n",
    "        # goal_pos = torch.stack(goal_vec)\n",
    "        train_valid = torch.stack(valids)\n",
    "        rnn_state = (rnn_state0[0], rnn_state0[1])\n",
    "        # ACNetLoss.forward(self, policy, value, actions, target_v, advantages)\n",
    "        total_loss = self.global_network.ACNetLoss.forward(policy, values, actions, target_v, advantages)\n",
    "        ## Update the global network with gradients \n",
    "        #policy, value, state_out, policy_sig = self.global_network.optimizer.step(inputs, water_res)?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # 以下步骤应该是在learning_agent当中使用\n",
    "        # Get gradients from local network using local losses and\n",
    "        # normalize the gradients using clipping\n",
    "        GRAD_CLIP = gamma\n",
    "        trainable_vars = torch.from_numpy(policy) # FIXME how to express?\n",
    "        gradients = self.global_network.torch.autograd.grad(total_loss, trainable_vars, create_graph=True)\n",
    "        var_norms = self.global_network.torch.norm(torch.cat([v.view(-1) for v in trainable_vars]))\n",
    "        grad_norms = self.global_network.torch.nn.utils.clip_grad_norm_(gradients, GRAD_CLIP)\n",
    "        apply_grads = self.global_network.trainer(self.global_network.parameters(), LR_Q)\n",
    "\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "    def shouldRun(self, episode_count):\n",
    "\n",
    "        return (episode_count < NUM_EXPS)\n",
    "    \n",
    "    def work(self, max_episode_length, gamma):\n",
    "    #(self, max_episode_length, gamma, sess, coord, saver):\n",
    "        global episode_count, episode_rewards, episode_lengths, episode_mean_values, episode_invalid_ops\n",
    "        total_steps, i_buf = 0, 0    \n",
    "\n",
    "        \n",
    "\n",
    " \n",
    "\n",
    "        while self.shouldRun(episode_count):\n",
    "                episode_buffer, episode_values = [], []\n",
    "                episode_reward = episode_step_count = episode_inv_count = 0\n",
    "                d = False\n",
    "\n",
    "                # Initial state from the environment for each time runs work()\n",
    "                s                     = self.env.reset()  #observe_space\n",
    "                rnn_state             = self.global_network.weights_init() #state init does not have output TODO\n",
    "                rnn_state0            = rnn_state\n",
    "\n",
    "                saveGIF = False\n",
    "\n",
    "                # Initial random action\n",
    "\n",
    "                for i in range(agent_num):\n",
    "                    a[0,i] = np.random.choice([0, 1, 3, 3, 4, 5])\n",
    "                    a[1,i] = np.random.choice([0, 1, 2, 3, 4, 5])\n",
    "\n",
    "                if OUTPUT_GIFS and ((not TRAINING) or (episode_count >= self.nextGIF)):\n",
    "                    saveGIF = True\n",
    "                    self.nextGIF = episode_count + 256\n",
    "                    GIF_episode = int(episode_count)\n",
    "                    episode_frames = [ self.env._render(mode='rgb_array',screen_height=900,screen_width=900) ]\n",
    "                    \n",
    "                while True: \n",
    "\n",
    "                    ##Take an action from network according to current obs.\n",
    "                    #~ , v, rnn_state, ~ = self.global_network.optimize_model()\n",
    "                    #obs = np.array(self.env.observe_space)\n",
    "                    #inputs_raw = np.array(obs[:, :4])\n",
    "                    #inputs = []\n",
    "                    #for map in inputs_raw:\n",
    "                    #    reshaped_map = [np.array(arr) for arr in map]\n",
    "                    #    inputs.append(reshaped_map)\n",
    "                    obs = np.array(self.env.observe_space)\n",
    "                    ##TODO get actions from obs?\n",
    "                    #ACNet return policy, value, state_out, policy_sig\n",
    "                    inputs_raw = np.array(obs[:, :4])\n",
    "                    inputs = []\n",
    "                    for map in inputs_raw:\n",
    "                        reshaped_map = [np.array(arr) for arr in map]\n",
    "                        inputs.append(reshaped_map)\n",
    "                    inputs = np.array(inputs)\n",
    "                    water_res = np.array(obs[:, 4]).reshape(-1,1)\n",
    "                    # run the net.forward to return self.policy, self.value, self.state_out, policy_sig\n",
    "                    policy, _, v,_ =  self.global_network.forward(inputs,water_res)\n",
    "                    actions = policy\n",
    "\n",
    "\n",
    "                    #environment operation and update\n",
    "                    s1, r, d = self.env.step(actions)\n",
    "                    #env.step will return self.observe_space, self.rewards, self.finished\n",
    "                          \n",
    "                    if saveGIF:\n",
    "                        episode_frames.append(self.env._render(mode='rgb_array',screen_width=900,screen_height=900))\n",
    "\n",
    "                    #episode_buffer.append([s[0],s[1],a,r,s1,d,v[0,0],train_valid])\n",
    "                    #episode_values.append(v[0,0])\n",
    "                    episode_buffer.append([s[0], s[1],actions, r, s1, d, v.data.cpu().numpy()[0, 0]])\n",
    "                    episode_values.append(v.data.cpu().numpy()[0, 0])\n",
    "                    episode_reward += r\n",
    "                    s = s1\n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "\n",
    "                    if d == True:\n",
    "                        print('\\n{} Goodbye World. We did it!'.format(episode_step_count), end='\\n')\n",
    "\n",
    "                    ## If the episode hasn't ended, but the experience buffer is full, then we\n",
    "                    ## make an update step using that experience rollout.\n",
    "                    if TRAINING and (len(episode_buffer) % EXPERIENCE_BUFFER_SIZE == 0 or d):\n",
    "                        ## Since we don't know what the true final return is, we \"bootstrap\" from our current value estimation.\n",
    "                        if len(episode_buffer) >= EXPERIENCE_BUFFER_SIZE:\n",
    "                            episode_buffer_training = episode_buffer[-EXPERIENCE_BUFFER_SIZE:]\n",
    "                        else:\n",
    "                            episode_buffer_training = episode_buffer[:]\n",
    "\n",
    "                        if d:\n",
    "                            s1Value = 0\n",
    "                        else:\n",
    "                            s1Value, _ = self.global_network(\n",
    "                                torch.from_numpy(s[0]), \n",
    "                                torch.from_numpy(s[1]), \n",
    "                                rnn_state)\n",
    "                        \n",
    "                        goal_vec = torch.from_numpy([])\n",
    "                        rollout     = np.array(s,goal_vec,actions,r,policy,episode_values)\n",
    "                        \n",
    "                        #v_l, p_l, valid_l, e_l, g_n, v_n = self.train(\n",
    "                        #train() will return policy, value, state_out, policy_sig\n",
    "                        ## update neural network weights in train()\n",
    "                        total_loss = self.train(\n",
    "                            episode_buffer_training, rollout, gamma, s1Value, rnn_state0)\n",
    "                        \n",
    "                        \n",
    "\n",
    "                        rnn_state0 = rnn_state\n",
    "\n",
    "                    if episode_step_count >= max_episode_length or d:\n",
    "                        break\n",
    "\n",
    "                episode_lengths.append(episode_step_count)\n",
    "                episode_mean_values.append(np.nanmean(episode_values))\n",
    "                episode_invalid_ops.append(episode_inv_count)\n",
    "                episode_rewards.append(episode_reward)\n",
    "\n",
    "\n",
    "                if not TRAINING:\n",
    "                    episode_count += 1\n",
    "                    print('({}) Thread 0: {} steps, {:.2f} reward ({} invalids).'.format(episode_count, episode_step_count, episode_reward, episode_inv_count))\n",
    "                    GIF_episode = int(episode_count)\n",
    "                else:\n",
    "                    episode_count += 1\n",
    "\n",
    "                    if episode_count % SUMMARY_WINDOW == 0:\n",
    "                        if episode_count % 100 == 0:\n",
    "                            print ('Saving Model', end='\\n')\n",
    "                        #    saver.save(sess, model_path+'/model-'+str(int(episode_count))+'.cptk')\n",
    "                            torch.save(self.global_network.state_dict(),\n",
    "                                   os.path.join(model_path, 'model-{}.pth'.format(int(episode_count)))\n",
    "                                   )\n",
    "                            print ('Saved Model', end='\\n')\n",
    "                        mean_reward = np.nanmean(episode_rewards[-SUMMARY_WINDOW:])\n",
    "                        mean_length = np.nanmean(episode_lengths[-SUMMARY_WINDOW:])\n",
    "                        mean_value = np.nanmean(episode_mean_values[-SUMMARY_WINDOW:])\n",
    "                        mean_invalid = np.nanmean(episode_invalid_ops[-SUMMARY_WINDOW:])\n",
    "\n",
    "                        ## Visualisation\n",
    "                        #summary = tf.Summary()\n",
    "                        #summary.value.add(tag='Perf/Reward', simple_value=mean_reward)\n",
    "                        #summary.value.add(tag='Perf/Length', simple_value=mean_length)\n",
    "                        #summary.value.add(tag='Perf/Valid Rate', simple_value=(mean_length-mean_invalid)/mean_length)\n",
    "\n",
    "                        #summary.value.add(tag='Losses/Value Loss', simple_value=v_l)\n",
    "                        #summary.value.add(tag='Losses/Policy Loss', simple_value=p_l)\n",
    "                        #summary.value.add(tag='Losses/Valid Loss', simple_value=valid_l)\n",
    "                        #summary.value.add(tag='Losses/Grad Norm', simple_value=g_n)\n",
    "                        #summary.value.add(tag='Losses/Var Norm', simple_value=v_n)\n",
    "                        #global_summary.add_summary(summary, int(episode_count))\n",
    "\n",
    "                        #global_summary.flush()\n",
    "\n",
    "                        #if printQ:\n",
    "                        #    print('{} Tensorboard updated'.format(episode_count), end='\\r')\n",
    "\n",
    "                if saveGIF:\n",
    "                    # Dump episode frames for external gif generation (otherwise, makes the jupyter kernel crash)\n",
    "                    time_per_step = 0.1\n",
    "                    images = np.array(episode_frames)\n",
    "                    if TRAINING:\n",
    "                        make_gif(images, '{}/episode_{:d}_{:d}_{:.1f}.gif'.format(gifs_path,GIF_episode,episode_step_count,episode_reward))\n",
    "                    else:\n",
    "                        make_gif(images, '{}/episode_{:d}_{:d}.gif'.format(gifs_path,GIF_episode,episode_step_count), duration=len(images)*time_per_step,true_image=True,salience=False)\n",
    "                if SAVE_EPISODE_BUFFER:\n",
    "                    with open('gifs3D/episode_{}.dat'.format(GIF_episode), 'wb') as file:\n",
    "                        pickle.dump(episode_buffer, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "#print(\"Hello World\")\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "#config = tf.ConfigProto(allow_soft_placement = True)\n",
    "#config.gpu_options.allow_growth=True\n",
    "\n",
    "if not TRAINING:\n",
    "    gifs_path += '_tests'\n",
    "    if SAVE_EPISODE_BUFFER and not os.path.exists('gifs3D'):\n",
    "        os.makedirs('gifs3D')\n",
    "\n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists(gifs_path):\n",
    "    os.makedirs(gifs_path)\n",
    "'''\n",
    "#with tf.device(\"/gpu:0\"): # uncomment to run on GPU, and comment next line\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    trainer = tf.contrib.opt.NadamOptimizer(learning_rate=LR_Q, use_locking=True)\n",
    "\n",
    "    net = ACNet(GLOBAL_NET_SCOPE, a_size, trainer, True, GRID_SIZE) # Generate global network\n",
    "\n",
    "    gameEnv = sapp_gym.SAPPEnv(DIAGONAL_MOVEMENT=DIAG_MVMT, SIZE=ENVIRONMENT_SIZE, \n",
    "                               observation_size=GRID_SIZE, PROB=OBSTACLE_DENSITY)\n",
    "\n",
    "    worker  = Worker(gameEnv, net)\n",
    "\n",
    "    global_summary = tf.summary.FileWriter(train_path)\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "\n",
    "        if load_model == True:\n",
    "            print ('Loading Model...')\n",
    "            if not TRAINING:\n",
    "                with open(model_path+'/checkpoint', 'w') as file:\n",
    "                    file.write('model_checkpoint_path: \"model-{}.cptk\"'.format(MODEL_NUMBER))\n",
    "                    file.close()\n",
    "            ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "            p=ckpt.model_checkpoint_path\n",
    "            p=p[p.find('-')+1:]\n",
    "            p=p[:p.find('.')]\n",
    "            if TRAINING:\n",
    "                episode_count = int(p)\n",
    "            else:\n",
    "                episode_count = 0\n",
    "            saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "            print(\"episode_count set to \",episode_count)\n",
    "            if RESET_TRAINER:\n",
    "                trainer = tf.contrib.opt.NadamOptimizer(learning_rate=lr, use_locking=True)\n",
    "\n",
    "        worker.work(max_episode_length, gamma, sess, coord,saver)\n",
    "'''\n",
    "## Create your PyTorch model and optimizer\n",
    "\n",
    "trainer = torch.optim.SGD\n",
    "# ACNet.__init__(self, a_size, batch_size, trainer, learning_rate, TRAINING, GRID_SIZE)\n",
    "net = ACNet(a_size, agent_num, trainer, LR_Q, TRAINING, GRID_SIZE)\n",
    "\n",
    "game_env = FFEnv(agent_num=agent_num)\n",
    "worker = Worker(game_env, net)\n",
    "global_summary = None  ## TODO Set up Tensorboard writer\n",
    "saver = None  ## TODO Set up PyTorch model saver\n",
    "\n",
    "\n",
    "if load_model:\n",
    "    print('Loading Model...')\n",
    "    if not TRAINING:\n",
    "        # Load PyTorch model\n",
    "        model_path = os.path.join(model_path, 'model-{}.pth'.format(MODEL_NUMBER))\n",
    "        checkpoint = torch.load(model_path)\n",
    "        net.load_state_dict(checkpoint)\n",
    "        net.eval()\n",
    "        episode_count = MODEL_NUMBER\n",
    "    else:\n",
    "        episode_count = 0\n",
    "    print(\"episode_count set to \", episode_count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if not TRAINING:\n",
    "    print([np.mean(episode_lengths), np.sqrt(np.var(episode_lengths)), np.mean(np.asarray(np.asarray(episode_lengths) < max_episode_length, dtype=float))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes decisions and enacts them on the environment (env.step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asks the NN for an output (policy) given the current input (obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records experience (state, action, reward) to update NN weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
