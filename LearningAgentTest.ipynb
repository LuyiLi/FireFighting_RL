{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.n as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.signal as signal\n",
    "import os\n",
    "import pickle\n",
    "import imageio\n",
    "\n",
    "import sapp_gym\n",
    "from ACNet_pytorch import ACNet\n",
    "from torchviz import make_dot\n",
    "import AgentState\n",
    "from FireFightingEnv import FFEnv\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "#model = \n",
    "#model = model.to(device)\n",
    "\n",
    "\n",
    "def make_gif(images, fname, duration=2, true_image=False,salience=False,salIMGS=None):\n",
    "    imageio.mimwrite(fname,images,subrectangles=True)\n",
    "    print(\"wrote gif\")\n",
    "\n",
    "def discount(x, gamma):\n",
    "    return signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning parameters\n",
    "max_episode_length     = 128\n",
    "episode_count          = 0\n",
    "EPISODE_START          = episode_count\n",
    "gamma                  = .95 # discount rate for advantage estimation and reward discounting\n",
    "#moved network parameters to ACNet.py\n",
    "EXPERIENCE_BUFFER_SIZE = 128\n",
    "GRID_SIZE              = 11 #the size of the FOV grid to apply to each agent\n",
    "ENVIRONMENT_SIZE       = (15,15)#the total size of the environment (length of one side)\n",
    "OBSTACLE_DENSITY       = (0,.3) #range of densities\n",
    "DIAG_MVMT              = False # Diagonal movements allowed?\n",
    "a_size                 = 4 + 1 + 4 + 1 # action size: 4 direction movements, stop, 4 direction spraying, and go back to water supply station.\n",
    "SUMMARY_WINDOW         = 10\n",
    "LR_Q                   = 8.e-5\n",
    "load_model             = False\n",
    "RESET_TRAINER          = False\n",
    "model_path             = 'model_sapp_vanilla'\n",
    "gifs_path              = 'gifs_sapp_vanilla'\n",
    "train_path             = 'train_sapp_vanilla'\n",
    "GLOBAL_NET_SCOPE       = 'global'\n",
    "agent_num              = 3\n",
    "\n",
    "# Simulation options\n",
    "FULL_HELP              = False\n",
    "OUTPUT_GIFS            = True\n",
    "SAVE_EPISODE_BUFFER    = False\n",
    "\n",
    "# Testing\n",
    "TRAINING               = True\n",
    "GREEDY                 = False\n",
    "NUM_EXPS               = 100\n",
    "MODEL_NUMBER           = 140500\n",
    "\n",
    "# Shared arrays for tensorboard\n",
    "episode_rewards        = []\n",
    "episode_lengths        = []\n",
    "episode_mean_values    = []\n",
    "episode_invalid_ops    = []\n",
    "rollouts               = None\n",
    "printQ                 = False # (for headless)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker:\n",
    "    def __init__(self, game, global_network):\n",
    "        self.env = game\n",
    "        self.global_network = global_network\n",
    "\n",
    "        self.nextGIF = episode_count # For GIFs output\n",
    "\n",
    "    def train(self, rollout, gamma, bootstrap_value, rnn_state0, imitation=False):\n",
    "        #tf:(self, rollout, sess, gamma, bootstrap_value, rnn_state0, imitation=False)\n",
    "        global episode_count\n",
    "\n",
    "        # [s[0],s[1],a,r,s1,d,v[0,0],train_valid]\n",
    "        rollout     = np.array(rollout)\n",
    "        observ      = torch.from_numpy(rollout[:,0])\n",
    "        #goal_vec    = torch.from_numpy(rollout[:,1])\n",
    "        actions     = torch.from_numpy(rollout[:,2])\n",
    "        rewards     = torch.from_numpy(rollout[:,3])\n",
    "        values      = torch.from_numpy(rollout[:,6])\n",
    "        valids      = torch.from_numpy(rollout[:,7])\n",
    "\n",
    "        # Here we take the rewards and values from the rollout, and use them to \n",
    "        # generate the advantage and discounted returns. (With bootstrapping)\n",
    "        # The advantage function uses \"Generalized Advantage Estimation\"\n",
    "        self.rewards_plus = torch.cat((rewards, bootstrap_value.unsqueeze(0)))\n",
    "        #self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "        discounted_rewards = discount(self.rewards_plus,gamma)[:-1]\n",
    "        self.value_plus = torch.cat((values, bootstrap_value.unsqueeze(0)))\n",
    "        #self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "        advantages = discount(advantages,gamma)\n",
    "\n",
    "        # Generate network statistics to periodically save\n",
    "        #feed_dict = {\n",
    "        #    self.global_network.target_v:np.stack(discounted_rewards),\n",
    "        #    self.global_network.inputs:np.stack(observ),\n",
    "        #    self.global_network.goal_pos:np.stack(goal_vec),\n",
    "        #    self.global_network.actions:actions,\n",
    "        #    self.global_network.train_valid:np.stack(valids),\n",
    "        #    self.global_network.advantages:advantages,\n",
    "        #    self.global_network.state_in[0]:rnn_state0[0],\n",
    "        #    self.global_network.state_in[1]:rnn_state0[1]\n",
    "        #}\n",
    " \n",
    "        # Modifing grad\n",
    "        self.global_network.torch.zero_grad()\n",
    "        target_v = torch.stack(discounted_rewards)\n",
    "        inputs = torch.stack(observ)\n",
    "        # goal_pos = torch.stack(goal_vec)\n",
    "        train_valid = torch.stack(valids)\n",
    "        rnn_state = (rnn_state0[0], rnn_state0[1])\n",
    "        \n",
    "        ## Update the global network with gradients \n",
    "        policy, value, state_out, policy_sig = self.global_network._build_net(inputs_batch, water_res_batch, a_size)\n",
    "\n",
    "        return policy, value, state_out, policy_sig\n",
    "    \n",
    "    def shouldRun(self, episode_count):\n",
    "\n",
    "        return (episode_count < NUM_EXPS)\n",
    "    \n",
    "    def work(self, max_episode_length, gamma):\n",
    "    #(self, max_episode_length, gamma, sess, coord, saver):\n",
    "        global episode_count, episode_rewards, episode_lengths, episode_mean_values, episode_invalid_ops\n",
    "        total_steps, i_buf = 0, 0        \n",
    "\n",
    "        while self.shouldRun(episode_count):\n",
    "                episode_buffer, episode_values = [], []\n",
    "                episode_reward = episode_step_count = episode_inv_count = 0\n",
    "                d = False\n",
    "\n",
    "                # Initial state from the environment\n",
    "                s                     = self.env.reset()  #observe_space\n",
    "                rnn_state             = self.global_network.state_init\n",
    "                rnn_state0            = rnn_state\n",
    "\n",
    "                saveGIF = False\n",
    "                if OUTPUT_GIFS and ((not TRAINING) or (episode_count >= self.nextGIF)):\n",
    "                    saveGIF = True\n",
    "                    self.nextGIF = episode_count + 256\n",
    "                    GIF_episode = int(episode_count)\n",
    "                    episode_frames = [ self.env._render(mode='rgb_array',screen_height=900,screen_width=900) ]\n",
    "                    \n",
    "                while True: \n",
    "\n",
    "                    ##Take an action from policy network output.\n",
    "                    a_dist, v, rnn_state = torch.rand(\n",
    "                        torch.from_numpy(s[0]), torch.from_numpy(s[1]), rnn_state)\n",
    "\n",
    "                    obs = np.array(self.env.observe_space)\n",
    "                    inputs_raw = np.array(obs[:, :4])\n",
    "                    inputs = []\n",
    "                    for map in inputs_raw:\n",
    "                        reshaped_map = [np.array(arr) for arr in map]\n",
    "                        inputs.append(reshaped_map)\n",
    "                    inputs = np.array(inputs)\n",
    "                    water_res = np.array(obs[:, 4]).reshape(-1,1)\n",
    "                   \n",
    "                    #env evolution\n",
    "                    s1, r, d = self.env.step(a)\n",
    "                    \n",
    "                    #env.step will return self.observe_space, self.rewards, self.finished\n",
    "                          \n",
    "                    if saveGIF:\n",
    "                        episode_frames.append(self.env._render(mode='rgb_array',screen_width=900,screen_height=900))\n",
    "\n",
    "                    #episode_buffer.append([s[0],s[1],a,r,s1,d,v[0,0],train_valid])\n",
    "                    #episode_values.append(v[0,0])\n",
    "                    episode_buffer.append([s[0], s[1], a, r, s1, d, v.data.cpu().numpy()[0, 0], valid_dist.data.cpu().numpy()])\n",
    "                    episode_values.append(v.data.cpu().numpy()[0, 0])\n",
    "                    episode_reward += r\n",
    "                    s = s1\n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "\n",
    "                    if d == True:\n",
    "                        print('\\n{} Goodbye World. We did it!'.format(episode_step_count), end='\\n')\n",
    "\n",
    "                    ## If the episode hasn't ended, but the experience buffer is full, then we\n",
    "                    ## make an update step using that experience rollout.\n",
    "                    if TRAINING and (len(episode_buffer) % EXPERIENCE_BUFFER_SIZE == 0 or d):\n",
    "                        ## Since we don't know what the true final return is, we \"bootstrap\" from our current value estimation.\n",
    "                        if len(episode_buffer) >= EXPERIENCE_BUFFER_SIZE:\n",
    "                            episode_buffer_training = episode_buffer[-EXPERIENCE_BUFFER_SIZE:]\n",
    "                        else:\n",
    "                            episode_buffer_training = episode_buffer[:]\n",
    "\n",
    "                        if d:\n",
    "                            s1Value = 0\n",
    "                        else:\n",
    "                            s1Value, _ = self.global_network(\n",
    "                                torch.from_numpy(s[0]), \n",
    "                                torch.from_numpy(s[1]), \n",
    "                                rnn_state)\n",
    "                        #v_l, p_l, valid_l, e_l, g_n, v_n = self.train(\n",
    "                        #train() will return policy, value, state_out, policy_sig\n",
    "                        policy, value, state_out, policy_sig = self.train(\n",
    "                            episode_buffer_training, gamma, s1Value, rnn_state0)\n",
    "                        \n",
    "                        \n",
    "\n",
    "                        rnn_state0 = rnn_state\n",
    "\n",
    "                    if episode_step_count >= max_episode_length or d:\n",
    "                        break\n",
    "\n",
    "                episode_lengths.append(episode_step_count)\n",
    "                episode_mean_values.append(np.nanmean(episode_values))\n",
    "                episode_invalid_ops.append(episode_inv_count)\n",
    "                episode_rewards.append(episode_reward)\n",
    "\n",
    "\n",
    "                if not TRAINING:\n",
    "                    episode_count += 1\n",
    "                    print('({}) Thread 0: {} steps, {:.2f} reward ({} invalids).'.format(episode_count, episode_step_count, episode_reward, episode_inv_count))\n",
    "                    GIF_episode = int(episode_count)\n",
    "                else:\n",
    "                    episode_count += 1\n",
    "\n",
    "                    if episode_count % SUMMARY_WINDOW == 0:\n",
    "                        if episode_count % 100 == 0:\n",
    "                            print ('Saving Model', end='\\n')\n",
    "                        #    saver.save(sess, model_path+'/model-'+str(int(episode_count))+'.cptk')\n",
    "                            torch.save(self.global_network.state_dict(),\n",
    "                                   os.path.join(model_path, 'model-{}.pth'.format(int(episode_count)))\n",
    "                                   )\n",
    "                            print ('Saved Model', end='\\n')\n",
    "                        mean_reward = np.nanmean(episode_rewards[-SUMMARY_WINDOW:])\n",
    "                        mean_length = np.nanmean(episode_lengths[-SUMMARY_WINDOW:])\n",
    "                        mean_value = np.nanmean(episode_mean_values[-SUMMARY_WINDOW:])\n",
    "                        mean_invalid = np.nanmean(episode_invalid_ops[-SUMMARY_WINDOW:])\n",
    "\n",
    "                        ## Visualisation\n",
    "                        #summary = tf.Summary()\n",
    "                        #summary.value.add(tag='Perf/Reward', simple_value=mean_reward)\n",
    "                        #summary.value.add(tag='Perf/Length', simple_value=mean_length)\n",
    "                        #summary.value.add(tag='Perf/Valid Rate', simple_value=(mean_length-mean_invalid)/mean_length)\n",
    "\n",
    "                        #summary.value.add(tag='Losses/Value Loss', simple_value=v_l)\n",
    "                        #summary.value.add(tag='Losses/Policy Loss', simple_value=p_l)\n",
    "                        #summary.value.add(tag='Losses/Valid Loss', simple_value=valid_l)\n",
    "                        #summary.value.add(tag='Losses/Grad Norm', simple_value=g_n)\n",
    "                        #summary.value.add(tag='Losses/Var Norm', simple_value=v_n)\n",
    "                        #global_summary.add_summary(summary, int(episode_count))\n",
    "\n",
    "                        #global_summary.flush()\n",
    "\n",
    "                        #if printQ:\n",
    "                        #    print('{} Tensorboard updated'.format(episode_count), end='\\r')\n",
    "\n",
    "                if saveGIF:\n",
    "                    # Dump episode frames for external gif generation (otherwise, makes the jupyter kernel crash)\n",
    "                    time_per_step = 0.1\n",
    "                    images = np.array(episode_frames)\n",
    "                    if TRAINING:\n",
    "                        make_gif(images, '{}/episode_{:d}_{:d}_{:.1f}.gif'.format(gifs_path,GIF_episode,episode_step_count,episode_reward))\n",
    "                    else:\n",
    "                        make_gif(images, '{}/episode_{:d}_{:d}.gif'.format(gifs_path,GIF_episode,episode_step_count), duration=len(images)*time_per_step,true_image=True,salience=False)\n",
    "                if SAVE_EPISODE_BUFFER:\n",
    "                    with open('gifs3D/episode_{}.dat'.format(GIF_episode), 'wb') as file:\n",
    "                        pickle.dump(episode_buffer, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "#print(\"Hello World\")\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "#config = tf.ConfigProto(allow_soft_placement = True)\n",
    "#config.gpu_options.allow_growth=True\n",
    "\n",
    "if not TRAINING:\n",
    "    gifs_path += '_tests'\n",
    "    if SAVE_EPISODE_BUFFER and not os.path.exists('gifs3D'):\n",
    "        os.makedirs('gifs3D')\n",
    "\n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists(gifs_path):\n",
    "    os.makedirs(gifs_path)\n",
    "'''\n",
    "#with tf.device(\"/gpu:0\"): # uncomment to run on GPU, and comment next line\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    trainer = tf.contrib.opt.NadamOptimizer(learning_rate=LR_Q, use_locking=True)\n",
    "\n",
    "    net = ACNet(GLOBAL_NET_SCOPE, a_size, trainer, True, GRID_SIZE) # Generate global network\n",
    "\n",
    "    gameEnv = sapp_gym.SAPPEnv(DIAGONAL_MOVEMENT=DIAG_MVMT, SIZE=ENVIRONMENT_SIZE, \n",
    "                               observation_size=GRID_SIZE, PROB=OBSTACLE_DENSITY)\n",
    "\n",
    "    worker  = Worker(gameEnv, net)\n",
    "\n",
    "    global_summary = tf.summary.FileWriter(train_path)\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "\n",
    "        if load_model == True:\n",
    "            print ('Loading Model...')\n",
    "            if not TRAINING:\n",
    "                with open(model_path+'/checkpoint', 'w') as file:\n",
    "                    file.write('model_checkpoint_path: \"model-{}.cptk\"'.format(MODEL_NUMBER))\n",
    "                    file.close()\n",
    "            ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "            p=ckpt.model_checkpoint_path\n",
    "            p=p[p.find('-')+1:]\n",
    "            p=p[:p.find('.')]\n",
    "            if TRAINING:\n",
    "                episode_count = int(p)\n",
    "            else:\n",
    "                episode_count = 0\n",
    "            saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "            print(\"episode_count set to \",episode_count)\n",
    "            if RESET_TRAINER:\n",
    "                trainer = tf.contrib.opt.NadamOptimizer(learning_rate=lr, use_locking=True)\n",
    "\n",
    "        worker.work(max_episode_length, gamma, sess, coord,saver)\n",
    "'''\n",
    "## Create your PyTorch model and optimizer\n",
    "\n",
    "trainer = torch.optim.SGD\n",
    "# __init__(self, scope, a, trainer, TRAINING, GRID_SIZE)\n",
    "net = ACNet(a_size, agent_num, trainer, learning_rate, TRAINING, GRID_SIZE)\n",
    "\n",
    "game_env = FFEnv(agent_num=agent_num)\n",
    "worker = Worker(game_env, net)\n",
    "global_summary = None  ## TODO Set up Tensorboard writer\n",
    "saver = None  ## TODO Set up PyTorch model saver\n",
    "\n",
    "\n",
    "if load_model:\n",
    "    print('Loading Model...')\n",
    "    if not TRAINING:\n",
    "        # Load PyTorch model\n",
    "        model_path = os.path.join(model_path, 'model-{}.pth'.format(MODEL_NUMBER))\n",
    "        checkpoint = torch.load(model_path)\n",
    "        net.load_state_dict(checkpoint)\n",
    "        net.eval()\n",
    "        episode_count = MODEL_NUMBER\n",
    "    else:\n",
    "        episode_count = 0\n",
    "    print(\"episode_count set to \", episode_count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if not TRAINING:\n",
    "    print([np.mean(episode_lengths), np.sqrt(np.var(episode_lengths)), np.mean(np.asarray(np.asarray(episode_lengths) < max_episode_length, dtype=float))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes decisions and enacts them on the environment (env.step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asks the NN for an output (policy) given the current input (obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records experience (state, action, reward) to update NN weights"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
