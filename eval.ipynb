{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ACNet_v1 import ACNet\n",
    "from ACNet_v1 import ReplayMemory\n",
    "import FireFightingEnv as FFEnv\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import torch.nn as nn\n",
    "import imageio\n",
    "\n",
    "AGT_COUNT = 3\n",
    "GRID_SIZE = 5\n",
    "\n",
    "def test_model(policy_net, env, num_episodes=10):\n",
    "    policy_net.eval()  # 将模型设为评估模式\n",
    "    fire_frames = []\n",
    "    fire_agent_frames = []\n",
    "    env_frames = []\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        while True:\n",
    "            # 对每个agent进行推断\n",
    "            actions = []\n",
    "            for s in state:\n",
    "                input = torch.from_numpy(s[0]).float().to(device)  # 将state转换为tensor\n",
    "                water_res = torch.tensor([[s[1]]]).float().to(device)\n",
    "                action_i = policy_net(input.unsqueeze(0), water_res).max(1)[1].view(1, 1)\n",
    "                action_int = action_i.item()\n",
    "                action = [action_int % 5, int(action_int / 5)]\n",
    "                actions.append(action)\n",
    "\n",
    "            # 执行动作\n",
    "            next_state, reward, done = env.step(actions)\n",
    "\n",
    "            # 更新总奖励\n",
    "            total_reward += sum(reward)\n",
    "\n",
    "            # 更新当前状态\n",
    "            state = next_state\n",
    "\n",
    "            fire_frames.append(env.mapEnv.figFire())\n",
    "            env_frames.append(env.mapEnv.figEnv())\n",
    "            fire_agent_frames.append(env.mapEnv.figFireAgent())\n",
    "            \n",
    "            \n",
    "            # 如果episode结束，则打印总奖励并退出循环\n",
    "            if done:\n",
    "                print(f\"Episode {i_episode + 1}, Total Reward: {total_reward}\")\n",
    "                imageio.mimsave('./results/gif/environment.gif', env_frames, duration=0.5)\n",
    "                # imageio.mimsave('./results/gif/fire.gif', fire_frames, duration=0.5)\n",
    "                imageio.mimsave('./results/gif/fire_agent.gif', fire_agent_frames, duration=0.5)\n",
    "                break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ACNet(\n",
       "  (conv1): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(64, 116, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc0): Linear(in_features=1, out_features=12, bias=True)\n",
       "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (lstm): LSTM(128, 128, batch_first=True)\n",
       "  (fc_policy): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (fc_value): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建环境\n",
    "env = FFEnv.FFEnv(3)  # 请用你实际使用的环境类来替代\n",
    "\n",
    "# 创建模型实例\n",
    "policy_net = ACNet(10, GRID_SIZE)  # 请用你实际使用的神经网络类来替代\n",
    "\n",
    "# 加载模型参数\n",
    "checkpoint = torch.load('./results/checkpoint/Run3.pth')  # 请用你实际的模型文件路径来替代\n",
    "policy_net.load_state_dict(checkpoint)\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy_net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: 111.70000000000002\n"
     ]
    }
   ],
   "source": [
    "TEST_ALL = 1\n",
    "if(TEST_ALL):\n",
    "    test_model(policy_net, env, num_episodes=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_VIEW_SINGLE_STEP = 0\n",
    "if(TEST_VIEW_SINGLE_STEP):\n",
    "    policy_net.eval()  # 将模型设为评估模式\n",
    "\n",
    "    state = env.reset()\n",
    "    total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_agent_frames = []\n",
    "\n",
    "if(TEST_VIEW_SINGLE_STEP):\n",
    "    # 对每个agent进行推断\n",
    "    actions = []\n",
    "    for s in state:\n",
    "        input = torch.from_numpy(s[0]).float().to(device)  # 将state转换为tensor\n",
    "        water_res = torch.tensor([[s[1]]]).float().to(device)\n",
    "        action_i = policy_net(input.unsqueeze(0), water_res).max(1)[1].view(1, 1)\n",
    "        action_int = action_i.item()\n",
    "        action = [action_int % 5, int(action_int / 5)]\n",
    "        actions.append(action)\n",
    "    print(actions)\n",
    "    # 执行动作\n",
    "    next_state, reward, done = env.step(actions)\n",
    "\n",
    "    # env.mapEnv.plotAgent()\n",
    "    # env.mapEnv.plotFireMap()\n",
    "\n",
    "    fire_agent_frames.append(env.mapEnv.figFireAgent())\n",
    "\n",
    "    # 更新总奖励\n",
    "    total_reward += sum(reward)\n",
    "\n",
    "    # 更新当前状态\n",
    "    state = next_state\n",
    "\n",
    "    # 如果episode结束，则打印总奖励并退出循环\n",
    "    if done:\n",
    "        print(\"DONE\")\n",
    "        imageio.mimsave('fire_agent.gif', fire_agent_frames, duration=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me5418-sb3-test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
